{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42844084",
   "metadata": {},
   "source": [
    "Test the API link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url='xxxxxxx',\n",
    "    api_key='xxxxxxx'\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0ba51",
   "metadata": {},
   "source": [
    "Test the API speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='xxxxxxx',  \n",
    "    api_key='xxxxxxx'\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Does this abstract mention AI methods: 'This paper uses machine learning...'\"}\n",
    "    ]\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"Time used: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb4b9b",
   "metadata": {},
   "source": [
    "Binary classification -  Determine the employment of AI methods based on publication abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from io import StringIO\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "\n",
    "# Initialize API client\n",
    "client = OpenAI(\n",
    "    base_url='xxxxxxx',\n",
    "    api_key='xxxxxxx'\n",
    ")\n",
    "\n",
    "# Read file and remove null characters\n",
    "input_csv = \"D:\\\\Resilience\\\\chatgpt test\\\\LR_clean.csv\"\n",
    "with open(input_csv, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    content = f.read().replace('\\x00', '')\n",
    "\n",
    "\n",
    "df = pd.read_csv(StringIO(content), low_memory=False)\n",
    "\n",
    "\n",
    "batch_size = 5             \n",
    "timeout_sec = 20           \n",
    "max_retries = 2           \n",
    "chunk_size = 1000          \n",
    "start_row = 1         \n",
    "end_row = 10001            \n",
    "\n",
    "# Classification function\n",
    "def classify_ai_use(idx, abstract_text):\n",
    "    if not abstract_text or str(abstract_text).lower() == \"nan\":\n",
    "        return idx, 0\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a scientific abstract classifier.\n",
    "\n",
    "Determine whether the following abstract **mentions the use of artificial intelligence (AI) methods**. AI methods include, but are not limited to:\n",
    "\n",
    "- Traditional machine learning (e.g., decision trees, support vector machines, k-nearest neighbors, logistic regression, random forest, XGBoost)\n",
    "- Deep learning (e.g., convolutional neural networks, recurrent neural networks, transformers, BERT, GPT, neural networks)\n",
    "- Natural language processing (e.g., word embeddings, text classification, language models)\n",
    "- AI tools/libraries (e.g., scikit-learn, TensorFlow, PyTorch, Keras)\n",
    "\n",
    "### Examples:\n",
    "1. \"This study uses convolutional neural networks to classify satellite images...\" → AI_use: 1  \n",
    "2. \"We employed logistic regression and random forest to analyze data...\" → AI_use: 1  \n",
    "3. \"This work evaluates environmental impacts using descriptive statistics and linear regression...\" → AI_use: 0  \n",
    "\n",
    "### Now classify the following abstract.\n",
    "Respond with **only** \"AI_use: 1\" or \"AI_use: 0\".\n",
    "\n",
    "Abstract: {abstract_text} \n",
    "\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an AI model classifying whether abstracts mention AI.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            result = response.choices[0].message.content.strip()\n",
    "            ai_use = 1 if \"AI_use: 1\" in result else 0\n",
    "            print(f\"Row {idx}: Success on attempt {attempt+1}\")\n",
    "            return idx, ai_use\n",
    "        except Exception as e:\n",
    "            print(f\"Row {idx}: Error on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(1)\n",
    "    return idx, \"ERROR\"\n",
    "\n",
    "# Loop through data in chunks\n",
    "for chunk_start in range(start_row, end_row, chunk_size):\n",
    "    chunk_end = min(chunk_start + chunk_size, end_row)\n",
    "    df_subset = df.iloc[chunk_start:chunk_end].copy().reset_index(drop=True)\n",
    "    ai_use_list = [None] * len(df_subset)\n",
    "\n",
    "    print(f\"\\nProcessing chunk {chunk_start} to {chunk_end - 1}...\")\n",
    "\n",
    "    for i in range(0, len(df_subset), batch_size):\n",
    "        batch = df_subset.iloc[i:i+batch_size]\n",
    "        with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "            futures = {\n",
    "                executor.submit(classify_ai_use, idx, str(row[\"Abstract\"]).strip()): idx\n",
    "                for idx, row in batch.iterrows()\n",
    "            }\n",
    "\n",
    "            for future in as_completed(futures, timeout=batch_size * timeout_sec):\n",
    "                idx = futures[future]\n",
    "                try:\n",
    "                    idx_result, result = future.result(timeout=timeout_sec)\n",
    "                    if 0 <= idx_result < len(ai_use_list):\n",
    "                        ai_use_list[idx_result] = result\n",
    "                except TimeoutError:\n",
    "                    print(f\"Row {idx}: Timeout\")\n",
    "                    if 0 <= idx < len(ai_use_list):\n",
    "                        ai_use_list[idx] = \"TIMEOUT\"\n",
    "                except Exception as e:\n",
    "                    print(f\"Row {idx}: Unknown error: {e}\")\n",
    "                    if 0 <= idx < len(ai_use_list):\n",
    "                        ai_use_list[idx] = \"ERROR\"\n",
    "\n",
    "        print(f\"Batch {i}-{i+len(batch)-1} completed.\")\n",
    "\n",
    "    # Add results column and save to file\n",
    "    df_subset[\"AI_use\"] = ai_use_list\n",
    "    output_path = f\"D:/Resilience/02Outputdataset/output_chatgpt_{chunk_start}-{chunk_end - 1}.csv\"\n",
    "    df_subset.to_csv(output_path, index=False)\n",
    "    print(f\"File saved: {output_path}\")\n",
    "\n",
    "print(\"All processing completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
